AWS DevOps Certification Training!

Start : 22nd Feb. 2025 - 1 Hr.

Actual Sessions starts from 1st March 2025

Sat-Sun - 7AM - 10AM IST.

AWS Free Tier Account 

Github Access


######################
Day 1 - 1st Mar. 2025
######################

	Pre-Requisites :
		- GitHub Account

		- AWS Free Tier Account 
		
		
	What is DevOps ?
	
	DevOps : Open-Source DevOps Services/AWS DevOps Services 
	
	What is DevOps ?
	
	SDLC : Software Development LifeCycle 
	
	Software / Applications :
	
		- Desktop Applications
		
		- Web Applications
		
		- Mobile Applications 
		
		- Embedded Applications
	
	What is your role as a DevOps Resource ?
	
	SDLC : Software Development LifeCycle 
	
		- Requirement Analysis 
		
		- Design and Documentation 
		
		- Coding and Development 
		
		- Testing 
		
		- Production Implementation 
		
		- Maintain/Monitor 
	
	
	Evolution of SDLC :
	
	Waterfall Model :
	
		- Top-Down / Linear Approach 
		- Once the Design is finalized we cannot change in between.
		- Was used to development Monolith Applications
		
	Eg.: 
	
		Desktop Applications
	
				-> Retail-Billing System-SuperMarket
				
						- Inventory Mgmt 		Features/Functions
						- User Interface 
						- Billing System 
						- Print the Bills 
						- Payment 
							- Cash/Card
							
		- Requirement Analysis 				- 10 Months 
		
		- Design and Documentation 
		
		- Coding and Development 		 - 4th Month 
		
		- Testing 
		
		- Production Implementation 
		
		- Maintain/Monitor	
		
		
	Agile Methodologies :
	
	
				-> Retail-Billing System-SuperMarket
				
						- Inventory Mgmt 			Features/Functions/Module => Iteration
						- User Interface 
						- Billing System 
						- Print the Bills 
						- Payment 
							- Cash/Card
							- new func1
							
		- Requirement Analysis 				- 10 Months 
		
		- Design and Documentation 
		
		- Coding and Development 		 - 4th Month 
		
		- Testing 
		
		- Production Implementation 
		
		- Maintain/Monitor			
				
	
	Iteration 1 : Inventory Mgmt
	
		- Requirement Analysis 		    
	    - Design and Documentation	    
	    - Coding and Development 		
        - Testing 		
        - Implemented to Production with proper approvals
		
	Iteration 2 : User Interface
	
		- Requirement Analysis 		    
	    - Design and Documentation	    
	    - Coding and Development 		
        - Testing 		
        - Implemented to Production with proper approvals
		
	Iteration nth : Any New Function
	
		- Requirement Analysis 		    
	    - Design and Documentation	    
	    - Coding and Development 		
        - Testing 		
        - Implemented to Production with proper approvals
		
	
	
		Using AGILE Methodologies we can able to achieve :
		
			- Continuous Development 
			- Continuous Integration 
			- Continuous Testing 
			- Continuous Delivery 
				- Is used to Release the product to Production Environment using Manual Approvals
			
		Using AGILE Methodologies, we cannot achieve :
		
			- Continuous Deployment 
				- Is used to Release the product to Production Environment without any Manual Approvals
			
			
	DevOps : SDLC 
	
	- Requirement Analysis 	
				
	- Design and Documentation 			
				
	- Coding and Development 				
			
	- Testing 			

	- Production Implementation 

	- Maintain/Monitor	
	
	What is DevOps ?
	
		DevOps is Software Development Strategy, that helps to promote the collaboration between the teams like Development Team and Operations Team to achieve Continuous Development, Continuous Integration, Continuous Testing, Continuous Delivery, Continuous Deployment and Continuous Monitoring in more automated fashion.
	
	How to Implement DevOps?
	
	Teams :
	
		Devops Team :
			Infra-Structure Management Team 		# Create Server/Config the Servers
			Application Development Team 
			Testing Team 
			Release Management Team 
			Production Support Team 				# 24/7 Support Team 
			Production Monitoring Team 
			IT Security Team 
			
	Environments :
	
	
		Non-Prod Environments												Prod Environments
		
			Dev Environment
			
			Build Environment
			
			Test Environment
				QA 
				
				UAT 					==================>						Production Servers 
			

	Application Architecture ::: 
	
		- Monolith Application Architecture
			- It is tightly Coupled Application Architecture
			- All the Dependencies/Functions are create with the single Application
			- We cannot split the dependencies easily
		
		
		- Micro-Service Based Application Architecture
			- It is loosely coupled 
			- Each and every functions are created as a independent service
			- Each Micro-Service will have its own source code repository to develop, test and implement.
			- Required to implement Continuous Deployment 
			
	DevOps Stages :
	
		Continuous Development :
		
			- It is the capability of any Application Development Team to Continuously Develop the Application Source Code.
			- It is used to Improve the Developers' Productivity.
			
			What is Role of Developers?
			
				- Create Source Codes 
				- Build the Code 		-- Compile the Code and Create Artifact(binaries - *.exec/*.war/*.jar/*.dll)
				- Perform Manual Unit Testing 
				- Promote the Changes to Higher Environment
				- Notify the Test Team thru EMails
				
			Using DevOps Approach :: 
			
				- Create Source Codes 
				- Save the Source Code in the resp. Source Code Repositories(GitHub)
				
				Automate :
				
				- Application Build 
				- Unit Testing 
				- Code Deployment/Promotion
				- Notification 
			
			Tools :
			
				IDEs - Integrated Development Environment : Eclipse, Visual Studio, Visual Studio Code, PyCharm
				Github/Git 
				AWS CodeCommit / GitHub 
				
			
		
		Continuous Integration :
			- It is the capability of any Application Development Team to Continuously Integrate the Changes for further Testing.


				Automate :
				
				- Application Build 
				- Unit Testing 
				- Code Deployment/Promotion
				- Notification 
				
			Tools :
			
				IDEs - Integrated Development Environment : Eclipse, Visual Studio, Visual Studio Code, PyCharm
				Github/Git 
				AWS CodeCommit / GitHub 	
				Jenkins/Docker/Kubernetes/Ansible							- Open-Source DevOps Tools 
				AWS CodeCommit/CodeBuild/CodeDeploy/CodePipelines			- AWS Managed Services 
				
			
		
		Continuous Testing :
			- It is the capability of any Testing Team to Continuously Test the Changes.
			- Automated Testing Tools :
				- TestNG/Selenium 
				
			- Test Cases/Test Data 
				

			Tools :
			
				Jenkins/AWS CodePipeline
			
		
		Continuous Delivery/Deployment :
		
			- Both are used to perform the Production Releases
			
			- Continuous Delivery :
			
				- It expects the Manual Approvals for Production Release 
				- During the process, there might be downtime 

			Scenarios :
			
				Eg.: Core Banking Applications :
				
					- Monolith
					
					Online - Banking / Credit Card Services
					
					Deployment Window :
						4Hrs - 6Hrs 		Release Window :
						
						Release New Version of the Product 
						
						Fix the Issue 
						Revert 

						
			- Continuous Deployment :
			
				- It never expects the Manual Approvals for Production Release 
				- During the process, there will not be any downtime in Production
				- Continuous Deployment can be achieved only with the Micro-Service Based Application Architecture
				
				
			Scenarios :
			
				Web applications :
				
					Facebook/Netflix/Amazon ......
					
					
					
		www.amazon.com :::
		
		Sign-Up			# Micro-Service1 - Developer1 - Create/Update Source-Code -> Unit Testing -> Promote to Test Environment -> Relased to Production
		Sign-In 		# Micro-Service2 - Developer1 - Create/Update Source-Code -> Unit Testing -> Promote to Test Environment -> Relased to Production
		Search 
		Add to Cart
		Place Order 
		Payment 
		Confirm the Details
		Confirm Order 
		Track orders
		
			Tools :
			
				--> Jenkins/Docker/Ansible/kubernetes/Terraform 
				--> AWS DevOps Services : AWS CodeCommit/Code-Build/Code-Deploy/Code-Pipeline/Code-Artifact
										  Cloudformation...
				
		
		Continuous Monitoring :
	
			- Is used to achieve business continuity..
			
			- Continuously Monitor the Infra-Structure and the Applications running in the Production Environment
			
			- Completely owned and managed by Monitoring Team. 
			
			- Infra-Structure Monitoring 
			
				-  Production Server :
				
						--> CPU Utilization 
						
						--> Memory Utilization 
						
						--> Network Traffic 
						
						
					Monitoring Tools :
					
						- Prometheus/Grafana/Splunk/Dynatrace/Kafka/Nagios
						- AWS - AWS CloudWatch 
						
							-> 75% of threshold Limit
							
							-> Alert the Users (Production Support/Stakeholder)
						
			
			- Application Monitoring ::
			
				--> AppDynamics / DataDog
			
			
Next :

		Infra-Structure Automation 
		DevOps Roles & Responsibilities 
		DevOps LifeCycle 
				
			

######################
Day 2 - 2nd Mar. 2025
######################			

	Infra-Structure Provisioning/Creating and Configuration!
	
	Infra-Structure As Code(IAC) Tools :
	
		- Provisioning/Creating Resources 	--> Terraform/AWS Cloudformation/ARM/Jenkins/AWS CodePipelines
			
		- Configuration Management 			--> Ansible/Chef/Puppet/Jenkins/AWS CodePipelines
			
	
	DevOps CICD Pipelines ::::
	
	DevOps LifeCycle/Workflow :
	
	-> Application Builds and Deployments 
	
	
		Using DevOps Approach :: 
		
			- Create Source Codes 
			- Save the Source Code in the resp. Source Code Repositories(GitHub)

	DevOps LifeCycle/Workflow :		


		Create CICD Pipeline : Stages :
		
		- SCM_Checkout
		- Application Build 
		- Unit Testing 
		- Sonarqube Analysis  # ensure code quality
		- Promote the Changes to Target Server(QA)
		- Automate QA Testing 
		- Promote the Changes to Target Server(UAT)
		- Automate UAT Testing
		- Promote the Changes to Target Server(PROD)
		- Automate PROD Testing 
		- Monitoring
			-> Feedback
				-> Shared with Application Team
					-> Analyze and Design the New Req.
					
	DevOps Team :	--> Tools --> github,jenkins,docker,kubernetes/Terraform/Ansible
	
		Roles and Responsibilities :
		
		DevOps Associate --> Sr.DA 
		DevOps Engg. --> Sr. DE
		DevOps Lead --> Sr. DL 
		DevOps Archi --> Sr. D.Arch
		DevOps Consultants 		

	SDLC ==> Waterfall - Agile - DevOps - DevSecOps - GitOps - AIOps - MLOps - SRE
	
	Open-Source DevOps Tools :
		- Github/Github Actions/GitLab,Jenkins,Gitlab-ci,Docker,Kubernetes,Terraform,Ansible,Monitoring Tool - Prometheus/Grafana
	
	Azure DevOps Services 
		- AzBoard,AzRepos,AzPipelines,AzTest,AzArtifacts
		- Azure Cloud Essential Skills
		
	
	AWS DevOps ***
	
	DevOps is Software Development Strategy, that helps to promote the collaboration between the teams like Development Team and Operations Team to achieve Continuous Development, Continuous Integration, Continuous Testing, Continuous Delivery, Continuous Deployment and Continuous Monitoring in more automated fashion.
	
	DevOps is all about :
	
		- Process / Strategy 
		- People 
		- Tools 
		
		
	Detailed DevOps Assessment :
	
	People ==> 
	Strategy
	Tools :
	
		AWS Cloud --> Platform as a Service (PAAS)
		
		Azure 
		
		Terraform 
		
		DevOps --> 
		
			- Open-Source ?
			- AWS DevOps Services ?
			
			
Next AWS DevOps Services ***

	SDLC Automation using AWS DevOps Services :
	
	AWS CodeCommit 
	AWS CodeBuild 
	AWS CodeDeploy 	
	AWS CodePipeline 
	
	--> Github Account https://github.com/
	
	--> AWS Free Tier Account https://aws.amazon.com/console/
	
	
			- Active Email_Id
			- Mobile Phone Number 
			- Active Credit/Debit Cards
			
			
		Login to AWS Console as a root user.
		
			- Email_ID 	# User Name 
			- Password
			
		IAM User 
			- User-ID 
			- Password 
			
		Billing Management 
				
	
	AWS CodeCommit 
	AWS CodeBuild 
	AWS CodeDeploy 	
	AWS CodePipeline
	AWS EBS 
	ECS 
	ECR
	EKS 
	Kubernetes 
	
	Distributed Version Control System :
	
		- github 
		- gitlab 
		- bitbucket 
		- AWS CodeCommit 
		- Azure Repos 
		
		
	Java Web Application : 
	
		Application Build :::
		
			- Maven/Gradle/Ant 
			
			- Maven --> pom.xml 	# Used to maintain the dependencies and plugins to perform build
					--> maven goals # Task to be perform 
						-> compile --> mvn compile  
						-> test    --> mvn test 		# compile and perform unit testing
						-> package --> mvn package		# compile, test and create artifacts in the target folder
						
						
						
					Build : It is a process of compiling the source code and create artifacts(*.exec/*.war)
					Unit Test 
					Promoted to Target Environments
					
			- Build Environment : 
				- jenkins_slaveNode/gitlab-ci/github-actions(runner)/AWS CodeBuild(EC2 Instance)/Azure(Azure Agent)
			
				VM --> Install the Build Tools - git,jdk,maven,docker
				
				AWS CodeBuild(EC2 Instance) 
				
					--> Static Provisioning  -- Manually Create the EC2 Instance & Install Require Build Tools
													Create the artifacts
					
					--> Dynamic Provisioning --> At Build Time we the EC2 Instance will be created and preform the build
												 Once the build process is completed Build Server will be automatically removed
												 --> Saved in a external storage - AWS S3 Bucket / EBS / github
					
		
		
			- Artifactory Library :
			
				--> Store the Application Artifacts 
				--> S3 bucket 
				
				--> Containerized Applications 
				
						-- Create an Application Image using Artifacts
						-- Deploy that image in target thru Container Registry.
						

Next :

	Create IAM User.
	IAM Groups 
	IAM Policies 
	IAM Roles 
	
	CodeBuild Project 
	CodeDeploy
	Code Pipeline 
		
######################
Day 3 - 8th Mar. 2025
######################			
		
	

	Create IAM User.
	IAM Groups 
	IAM Policies 
	IAM Roles 
	
	CodeBuild Project 
	CodeDeploy
	Code Pipeline
		
		
		Free Tier Account --> 12 Months 
		
		VMs - 750Hrs/Month 
		
		10 VMs - 75Hrs/Months 
		
	AWS Billing Management Console :
	
	
		As a root user --> Billing and Cost Management Console 
		
		Choose free Tier 
		
	AWS Regions :
	
		- AWS Availability Zones (Data Centers)
			
			- Terraform / AWS Cloudformation Templates 		# Automate Resource Provisioning
			
			
	IAM Console :
			
	Create IAM User				# Any User who expects access to AWS Console/Resources 
	IAM Groups 					# IAM User Groups 
	IAM Policies 				# Define the permissions - to ensure User Authorization
								# Policies can be directly assign to any IAM User/Group 
								# It can be assigned to any IAM Role 
	IAM Roles 					# Define the profiles used to access between any resources 	
								# Role can be assigned to any AWS Services
	
	
	AWS Account Owner --> root user. (Account/Enterprise Level) - Complete Admin Access  
		- Email_ID 
		- Password 
		
	IAM Users 			--> Users to work with AWS Resources --> to ensure User Authentication
	
	IAM Groups :
	
		developer1,2,3,4,5 User ID
			- Access to a CodeBuild Project, CodeDeploy, Code Pipeline
			
		dev_user_group1						# Create a user group 
			developer1,2,3,4,5 User ID		# Add all the user to this group 
			
			Assign the required access at the group level. Access to a CodeBuild Project, CodeDeploy, Code Pipeline
			
		RBAC : - Role Based Access Control 
		
	IAM Roles :					# Define the profiles used to access between any resources
								# It composed of IAM Policies

		Virtual Machine(EC2 Instances)
			--> Create Application Build 
			--> Create Application Artifacts --> *.war 
			
			--> S3 Bucket 		
			
			To assign Access to any AWS Service to access.
			
	To Create IAM User/Group/Policies/Role # First login to AWS Console as a root user. 
	
	Create : IAM User/Group/Policies/Role
	
	https://aws.amazon.com/console/						# Use to login as a root user
	
	Create User Group and add the policies :
	
			AdministratorAccess
			AmazonS3FullAccess
			AWSCodeBuildAdminAccess
			AWSCodeCommitFullAccess
			AWSCodeDeployFullAccess
			AWSCodePipeline_FullAccess
			
			
	AWS CodeCommit :
		- It is used to manage the application source code.
		- Is a Distributed Version Control System
		- Alternative : github/gitlab/bitbucket
		
		
	Application Build :
	
		- Source Code Repo						github
		- Compile the Code and create artifacts
	
	
	AWS CodeBuild :
	
		- Used to perform Application Build - compile and create artifacts 
		
		- Build Tools :
		
			Java based web application :	EC2 Instance 
			
				- maven/gradle 
				
				- maven goal : clean package 	# Compile,Unit Test,Create Artifacts
				
				
				- pom.xml 						# Project Object Model 
				
				
				- buildspec.yaml 				# Custom build specifaction script 
												# Created by the developers 
												# Maintained as part of Source Code Repositories
				
			Create Artifacts --> will be saved in S3 Bucket 
	
	Application Project:	
	
		Java Springboot Web Application :
		
			Java-Web-App-Project1:			Project Root Folder 
			
				src 
					main 				# Business Logics
						*.java 
						*.java 
					
					test 				# Test Cases 
						*.java 
						*.java
				
				target/ 
				
				app.properties 
				
				security_cred
				
				app.config 
				
				pom.xml 				# Define the properties of Application Build 
										# Maintains the Dependencies and plugins required to perform Application Build
		
				buildspec.yaml 			# Required for CodeBuild Project 
										# This Defines the build tools and goals to be executed in build server dynamically.
				
				
				
			Maven Goal: 
			
				mvn clean package 
				
				mvn compile 			# Compile the Source 
				
				mvn test 				# Perform Code Compilation & Unit Testing 
				
				mvn package 			# Perform Code Compilation, Unit Testing and Create Artifacts in target folder 
				
				mvn clean				# Clean the target folder 
			
			
			Build Server :	
			
				--> git,jdk,maven 

	
	Code Build:	
	
		- SCM_Checkout in the Build Server(Dynamic EC2_Instance)	
		
		https://github.com/Edu-AWS-Devops-Mar-2025/devops-java-webapp.git
		
		- Read the buildspec.yaml from the source code repository
		
		- Install and configure the build tools 
		
		- Create the Artifacts 
		
		- Save the artifacts in the s3 bucket.
		
	Jenkins --> CodeBuild/CodeDeploy/CodePipeline 
	
	
	1. Prepare Java Maven Application Source Code Repository in GITHub
	
	2. Create S3 Bucket 
	
	3. Create a folder in S3 Bucket 
	
	4. Create CodeBuild Project using GitHub Repository and S3 as input references.
	
	
	Overview of DevOps CICD Pipeline ::::
	
		- SCM_Checkout								# CodeBuild Service			1 EC2 Instance (Created Dynamically)
		- Application Build 						# CodeBuild Service
		- Unit Testing 								# CodeBuild Service
		- Create Artifacts							# CodeBuild Service
		- Save Artifacts in S3 Bucket 				# CodeBuild Service
		
		
		- Promote the Changes to Target Server		# CodeDeploy Service		1 EC2 Instance 
		
		
		- CodePipeline is an integration of CodeCommit/GitHub, CodeBuild and CodeDeploy 
		
	Environments ::
	
	Non-Prod 																Prod-Environments
	
		Dev 
			# Prepare source code 
			# Push the source code to remote git repository
		
		Build 
			# Compile and Create Artifacts
			# Save the Artifacts in Artifactory Library (S3 Bucket)
		
		QA 
			# Download the artifacts from s3 bucket 
		
		UAT 														===> 			Prod Server 



Next :

	Create CodeBuild Project 
	
	Create CodeDeploy 


######################
Day 4 - 9th Mar. 2025
######################		

	
	Create CodeBuild Project :::
	
	Continuous Deployment ::: 	Micro-Service Based Application Architecture
								Containerized Deployment
								
	Containerization ::: 						
								
								
								
	Build_Server :
	
		--> EC2 Instance :
		
				-> CodeBuild Agent 
				
				-> jdk 
				
				-> maven 
				
				-> git 
				
	AWS Cli ==> Create CodeBuild 
	
		Build Application : Self Hosted Build Server 	-> Use this as a build Server.
		
							AWS Managed Build Server 	-> (Dynamic Creation during build, And gets deleted after build)
							
							
	Target Server :
	
		--> EC2 Instance :
		
				-> CodeDeploy Agent
		
				-> jdk 
				
				-> Tomcat
				
				
	
	
	CodeBuild Service :
	
		-> EC2 Instance 
		
			-> CodeBuild Agent 
								
								
	Jenkins_Master 
	
		Jenkine_Slave (Build_Server)
		
		
		
	Containerization :::		
	

	Open-Source: 		AWS: 		Azure: 		GCP:


	Docker Engine 		ECS 		ACS 		GCE 
	                                        
	DockerHub           ECR     	ACR         GCR 
	                                        
	Kubernetes          EKS     	AKS         GKE 
	
	
	Containerization :::
	
		-> It is a process of packaging the Application along with its dependencies.
		
	
	Developers' perspective :
	
		Dev Environment (Local Machine) :
		
			Create Source-Code (Visual Studio Code)
			
			Push the Changes to Github Repo
		
			Build & Create Artifacts ==> mywebapp.war 
			
			Unit Level Testing using Openjdk_17,Tomcat_Server_V8.5
			
			
			Package : (mywebapp.war,openjdk17,Tomcat_Server_V8.5)	==> Application_Container_Image ==> mywebapp_img:v1.0
			
			
			Publish the mywebapp_img:v1.0 to Container Registry(DockerHub/ECR/ACR/GCR)
			
			
			###Promote the *.war to target server
			
			
		QA_Server 		==> 	mywebapp.war 
		
			Install Docker Container-Engine 
		
			pull mywebapp_img:v1.0 from Container Registry(DockerHub/ECR/ACR/GCR)
		
			- openjdk17, Tomcat8.5
		
		UAT_Server 		==> 	mywebapp.war
		
			- jdk, Tomcat 		
		
		
		PROD_Server 	==> 	mywebapp.war
		
			- jdk, Tomcat 	
	
	
	How to Create Container Image ??
	
	
	
		1. Install Docker Container-Engine in the Build Server 
		
		
		2. Docker CLI Commands to interact with Container Engine and Create Container Images and Containers.
		
		
		Create Container Images: 
		
			- docker commit
				
				-> Used to create an image based on the existing Container reference
				-> Used to Create Container/Server Templates 
				-> Created by the infra-team 
					
				
			- docker build 
			
				-> Used to create an image based on the Dockerfile reference
				-> Dockerfile will be created by the Developers and maintain the source code repository
	
	
				Github Repos :					Artifactory Libraries/Jfrog/S3			DockerHub/ECR/ACR/GCR
	
				mywebapp.java_v1.0				mywebapp.war_V1.0						mywebapp_img:V1.0
				mywebapp.java_v1.1				mywebapp.war_V1.1               		mywebapp_img:V1.1
				mywebapp.java_v1.2				mywebapp.war_V1.2               		mywebapp_img:V1.2
				mywebapp.java_v1.3				mywebapp.war_V1.3               		mywebapp_img:V1.3
				mywebapp.java_v1.4				mywebapp.war_V1.4               		mywebapp_img:V1.4

		
		Dockerfile :::
		
		
		Terminologies related to Docker:
		
		
			- Containerization 				# It is a process of packaging the Application along with its dependencies.
			
			- Container Engine				# Used to Create and manage the Container Images and Containers 
			
			- Docker Cli Command			# Used to interact with Container Engine 
			
			- Container Image 				# It is static file that defines the Properties & Dependencies of the Container
											# Non-Executable
											# It compose of Dockerfile Instructions called as Layers of Container Image.
											
			- Container 					# Is an executable unit of Container Image 
											# Instance of Container Image 
			
			- Container Registry 			# It used to Save/Version Control the Container Images 
											# DockerHub -- Account 
											# https://hub.docker.com/
			
			- Container Repositories		# Sub-sets of Container Registry
			
			
		Dockerfile Creation :::	


			- Install Docker-Engine 
			
				sudo -i 
				
				apt install docker.io -y				# https://docs.docker.com/engine/install/
			
			- Using Docker CLi Commands 
			
				docker images					# To list the images in the local machine  
				
				docker ps 						# To list all the Active/Running process/Container 
				
				docker ps -a 					# To list all the Active and stopped/Exit Container 
				
		
		Property of any Container :
		
		
			- Container are considered as a OS level Virtualization.
			
			- Containers are created using Container-Engine.
			
			- Containers is used to execute any task/application. 
			
			- Containers are not used to execute an Operating System 
			
			- If there is no Task/Application, or if the task/application gets completed, The Containers will go to exit state.

			- Container are used to reduce the no. of VMs. and save Infra-Structure Cost. 
			
			- Container runs in its own isolated address space
			
				-> Kernel (Core of Linux Operating System)
				
					-> Namespaces 					
					-> Control Groups 
				
				
			
			
			- Virtual Machine :
			
				- Install Container Engine 
				
						- C1,2,3,4,5,6,7,8,9,....................,n 
						
			
			
				- Create Container Images 	
					
						docker commit 
						
						docker build 
						
						
						
			- Using Docker CLi Commands 
			
				docker images					# To list the images in the local machine  
				
				docker ps 						# To list all the Active/Running process/Container 
				
				docker ps -a 					# To list all the Active and stopped/Exit Container 
				
				
				docker pull <image_name>		# Download a container image from DockerHub Container Registry	to local machine
				
				docker run	<image_name>		# To Create Container based on the Image 	

				Modes of Docker Run :::
				
					1. Fore-ground/Attached Mode 		# Default Mode of Container Executation.
														# Terminal will be reserved to that Container
					
					2. Back-ground/Detached Mode		# To Execute Containers in Back-ground
				
					3. Interactive Mode 				# Login to Execute
					
	
Next :

	Work with Container Engine 
	
		- docker Commit / build 
		
		- push to DockerHub / ECR 
		
	Enhance Code Build Project using Container Images 

	Code Deploy 


######################
Day 5 - 15th Mar. 2025
######################
	
	Code Build :::
		
	Code Deploy :::
	
	Code Pipeline :::
	

	DevOps Workflow :
	
		Build Orchestration Tools :
		
			- Jenkins							java - groovy scripting language
			- gitlab-ci 						Declarative Scripts - *.yaml
			- bamboo 
			- Azure Pipelines 
			- GitHub-Actions
			- AWS CodePipeline 
			
		Jenkins Architecture :
		
			Jenkins_Master(VM) 			--> Create Jenkins Pipeline Projects & Schedule to run in the Build Agents(Master/Slave)				
			
				Jenkins_Slave_Nodes(VM)	--> Used to execute the Jenkins Pipeline Projects
				
		
		Scenario 1 :
		
			Java Based Application : Maven 
			
			Build Frequency / Release Frequency : 4 Builds/w & 2 Releases/m 
			
			
			Jenkins_Master(VM) 			--> Create Jenkins Pipeline Projects & Schedule to run in the Build Agents(Master)
										--> git,jdk,jenkins,Maven,docker 
										
										
		Scenario 2 :
		
			Build Frequency / Release Frequency : HIGH !

			Multiple Programming Languages to Deploy :
				
			
			Jenkins_Master(VM) 			--> Create Jenkins Pipeline Projects & Schedule to run in the Build Agents(Slave_Node)				
										--> git,jdk,jenkins
				
				Jenkine_Slave_Node1(VM)	--> Java_Build 
				Jenkine_Slave_Node2(VM)	--> Python 
				Jenkine_Slave_Node3(VM)	--> .Net_Build 
				Jenkine_Slave_Node4(VM)	--> Angular_Build 
				Jenkine_Slave_Node5(VM)	--> NodeJS_Build 
				Jenkine_Slave_Node6(VM)	--> Ruby_Build 		
				
	
		Containers :
		
		Infra-Structure Perspective :
		
			Infra-Structure Management Team :
			
				- Provision the Infra-Structure 		# Create Servers & Resources 
				
				- Configure Infra-Structure 			# Setup/Install/Uninstall/Upgrade the Tools and services in the Servers 
		
			
		- Containerization 				# It is a process of packaging the Application along with its dependencies.
										# To reduce the no. of VMs.
										# Container are used to reduce the no. of VMs. and save Infra-Structure Cost. 
		
		
			Jenkins_Master(VM) 			--> Create Jenkins Pipeline Projects & Schedule to run in the Build Agents(Slave_Node)				
										--> git,jdk,jenkins
										
				Jenkins_Build_Agent(VM) 
					- Install Container Engine 
							C1				--> Java_Build
							C2 				--> Python 
							C3              --> .Net_Build 
							C4              --> Angular_Build 
							C5              --> NodeJS_Build 
							C6              --> Ruby_Build 	
							
							
					- Server Templates :::
					
					
		How to Create Container Images :::
		
			--> docker Commit / build ::
			
			
			--> Docker Commit : 	# It is used to Create a docker container image based on the existing Container Reference 
									# Used to Create kind of Server template to build server
									
			--> Docker Build :		# It is used to Create a docker container image based on the Dockerfile Instructions
									
									
									
		Use Case 1 :	To Create Java Build Environment - Container (git/jdk/mvn)
		
			- Create a Contianer
				
					docker run -it ubuntu bash 					# Always create a Container 
			
			- Start Container 
			
					docker ps -a 
			
					docker start <container_id>
			
			- Login to running Container
				
					docker exec -it <container_id> bash 		# Login to the existing/running Contianer 
			
			- Stop the Container 
									
					docker stop <container_id>
					
		
		Create a New Container Image using the Existing Container reference :::
		
			- docker commit Command :::
		
				Syntax :
				
					docker commit <existing_Contiainer_ID> <Container_Repository_Name>/<New_Container_Image_Name>:<Tag>
					
					# Container_Repository_Name --> Is the DockerHub Repository Name
					
					# https://hub.docker.com/
		
		
				docker commit 6da9b479e29f loksaieta/myjavabsimage:v1.0
				
				
				
		Docker Push :::
		
			- To Push the Docker Image from Local Machine to Dockerhub Registry 
			
			
				- Login to DockerHub using Docker CLI 
				
					- UserName 
					
						docker login -u loksaieta
					
					- Password --> PAT to be created in DockerHub 
					
						sdfasdfasdfasdfasdf
						
		
			docker push <Container_Repository_Name>/<New_Container_Image_Name>:<Tag>
			
			docker push loksaieta/myjavabsimage:v1.0
									
			
		
		
		Docker Build :		# It is used to Create a docker container image based on the Dockerfile Instructions
							# Used by developers to create application images

			Dockerfile :::

			- Container Image 				# It is static file that defines the Properties & Dependencies of the Container
											# Non-Executable
											# It compose of Dockerfile Instructions called as Layers of Container Image.
											# Dockerfile will be created and maintained in the source code repository by the developers
			

			vi Dockerfile
			
			FROM ubuntu 
			RUN apt update -y 
			RUN apt install git -y 
			RUN apt install maven -y 
			
			save 
			
			docker build -t <Container_Repository_Name>/<New_Container_Image_Name>:<Tag> .			# Dockerfile reference
			
			
			Dockerfile Instructions :::
			
			
				FROM 				# To identify the Base Image 
				
				RUN 				# Execute any package 
				
				WORKDIR				# Set the Current Working Directory 
				
				ENV 				# Set the Environment Path Variables
				
				COPY 				# To Copy the files from the host volume to Container Volume 
				
				CP					# To Copy the files within the Container Volumes 			
				
				ADD 				# To Copy the files from the external URL(S3 URL) as well as from the host volume to Container Volume 
				
				EXPOSE				# Set the default Container Port 
				
				CMD 				# Set the start-up Task to the Container 
									# This Start-up Task/Command can be changed at runtime								
									
				ENTRYPOINT			# Set the start-up Task to the Container  
									# This Start-up Task/Command cannot be changed at runtime
									
									
				
		CICD Pipeline :

			Using CodeBuild :::: Containerize the Web Application Services

			
			FROM tomcat:8.0
			COPY ./target/loksaieta.war /usr/local/tomcat/webapps
			EXPOSE 8080
			
			
			
		CI/CD 
		
			- Build Pipeline 		
			
				SCM_Checkout
				Application Build(Create Artifacts)
				Application Image Build(Dockerfile)
				Publish Application Image to Container Registry
				
			
			- Release Pipeline 
			
				Pull the Application Image from container Registry
				Execute the Application using Containers 
				
		
		CICD Pipeline ::
		
				SCM_Checkout
				Application Build(Create Artifacts)
				Application Image Build(Dockerfile)
				Publish Application Image to Container Registry		
				
				Pull the Application Image from container Registry into QA Server
				Execute the Application using Containers 			
				
				Pull the Application Image from container Registry into UAT Server
				Execute the Application using Containers 	
				
				Pull the Application Image from container Registry into PROD Server
				Execute the Application using Containers 				
		
		
		How to Manage the Container Data ???
		
		
			Containers ==> are not the permanent entities!
			
			Container are used just to run the application. Not to save any data!
			
			Container Volumes mount.
			
			Stateful / Stateless Applications 
			
			By Default, Containers are meant for Stateless Applications 
			
			Stateless Applications 		# It will never leave a state of execution 
			
			stateful Applications		# It will leave a trace of execution(logs/reports/files/arguement/datafiles)
			
			
			
			
			3-Tier Applications:
			
				-> Front-End Layer 		# User Interaction
				
				-> Application Layer 	# Business Logics 
				
				-> Back-End Layer 		# Database 
		
		
			www.amazon.com 
			
			
			sign_in service 
	
				-> Front-End Layer 		# User Interaction		C1 
				
				-> Application Layer 	# Business Logics 		C2 
				
				-> Back-End Layer 		# Database 				C3 
				
				
		
			Docker Volume :::
			
			S3 Bucket as a Storage 
			
			Kubernetes ::: 
			
			
			ECS / EKS 
			
			ACS / AKS 
			
			GCE / GKE 
	
	
		CI/CD Pipeline :::
		
		Kubernetes :::

			- Kubernetes is an Open-Source Container Orchestration Tool 
			- To ensure high availability of Containerized Applications
			- By create replicas
			- Auto-Scale : Scale-up/Scale-Down
			- Load-Balancing 
			- Self-Healing 

			Managed/Paid Kubernetes Services from Service Providers :
			
				ECS / EKS 
				
				ACS / AKS 
				
				GCE / GKE 		

				- 3 Replicas of Containers 
				
			sign_in service 
	
				-> Front-End Layer 		# User Interaction		C1.1,C1.2,C1.3
				
				-> Application Layer 	# Business Logics 		C2.1,C2.2,C2.3
				
				-> Back-End Layer 		# Database 				C3.1,C3.2,C3.3 	
				
				
				
				Kubernetes-Master 
				
					- Kubernetes-WorkerNode1,2,3,4,5


	
########################
Day 6 - 16th Mar. 2025
########################	

	Code Build/Deploy/Pipeline ::::
	
	Containerization 
	
	Container Orchestration Tool - Kubernetes 
	
	Build Pipeline :
	
		Github ==> Build Agents/ECS/ECR/ ==>	Application readiness for deployment			*.war 
	
	Release Pipeline :
	
		Build Artifacts(S3)/ECR ==>	Target Server(Deployment_Server/Kubernetes_Cluster)
	
	
	Environments :

		Non-Prod												Prod-Environments
	
			Dev 
			
			Build 
			
			Target Environments (Kubernetes)
			
				QA 
				
				UAT 					============>				Prod Server1,2,3,4,5 (Deployment Group) 	# Non-Containerized Apps
	
	
																	Kubernetes-Cluster 							# Containerized Apps
																		Kubernetes-WorkerNode1,2,3,4,5
	
	
	Kubernetes-Master(Non-Prod)										Kubernetes-Master(Prod)
		Kubernetes-WorkerNode1,2,3,4,5                              	Kubernetes-WorkerNode1,2,3,4,5
			QA/UAT
			
	Difference between Open-Source & EKS :
	
		EKS : GUI based Interface , Kubernetes-Dashboards, In-build-Monitoring (AWS-Cloud-Watch), Auto-Scaling Group, Load-Balancer Service
			
		Open-Source K8s :
		
			: Dedicated Kubernetes Resources, Command Line Utilities - Kubeadm,kubectl, 
					
					Additionally we need to setup Kubernetes-Dashboards, In-build-Monitoring, Auto-Scaling Group - HELM Charts
					
					Load-Balancer Service(Service Provider - AWS/Azure/GCP) / Create Own On-Prem Load-Balancer using Nginx
					
					
	
	Resources (Target Environments) are Created ?
	
	
		- IAC Tools : Infra-Structure As Code!
		
			- CloudFormation is an IaC Tool - meant for AWS specific resources. 
			
			- Declarative scripts --> *.yaml / *.json 
			
			
		- Terraform is an alternative support multi-cloud/Hybrid resource provisioning!
		
		
		AWS CloudFormation :
		
			- Provision 
			
			- Configuration 
			
			- Automated Rollback
			
			- Resource Termination
			
		
		AWS CF using Cloudformation Templates & Cloudformation Stacks 
		
			
		CF Templates : To define the desired state of the Infra-Structure written in *.yaml / *.json --> keys:values Pairs 
					
					
		CF Stack : 	Is a collection of resources can be executed as a single unit. 
		
					It helps to preview the resources implement some run time parameters 
					
					Able to control/manage/update/execute & Terminate the resources 
	
	
					How to Access : AWS Console(GUI), AWS CLI, AWS SDK 
					
		CF Nested Stacks :
		
			CF Template1 : Create VPC 
			
			CF Template2 : Create EC2 DependsOn
			
		CF StacksSets :
			
			Deploy the CF Template in different AWS Regions 
			
			
		All the CF Templates are saved in s3 Bucket(by Default) / Github Repo 
		
		
		CF Functions : To Automate / Manage the CF Templates 
		
			Fn::getAtt:
			
			
		Python Scripts : Create AWS Resources boto3 Python Library!
		
		
		
		CF Framework/Process Workflow :
		
		
			1. Create CF Template	--> Stored in S3 Bucket		--> 	Create CF Stack and Execute 
			
	
		CF Template Sections : --> Building Blocks to Create CF Templates
		
		
		Format Version 
		
		Resources 	
		
		Description 
		
		MetaData & Parameters 		# To customize the input parameters 
		
		Mappings 					# Map any resources
		
		CFT Functions 
		
		CFT Conditions
		
				DependsOn 
				
					VPC,DependsOn - Subnet,DependsOnRT,DependsOnRTA,InternetGateway,security_group,EC2_Instance
	
				Create  a resource of for prod envi. 
			
				variable 
		
		Transform / Rules  			# Validation of of Parameters  
		
		
		TroubleShooting the CF Template Stack!
		
		Cloud Watch Monitoring Logs 
		
		To Create CF Templates --> 	Use Visual Studio Code IDE 
									AWS CF Extension for VS Code 
{
    "AWSTemplateFormatVersion": "2010-09-09",
    "Description": "Creating a new S3 bucket",
    "Resources": {
      "s3Bucket": {
        "Type": "AWS::S3::Bucket",
        "Properties": {
          "BucketName": "edu-16-s3"
        }
      }
    }
}									
									
		GitOps:
		
			Resources:
			  Bucket:
				Type: AWS::S3::Bucket
				Properties:
				  BucketName: !Sub ${AWS::StackName}-bucket-${AWS::AccountId}
				  BucketEncryption:
					ServerSideEncryptionConfiguration:
					  - ServerSideEncryptionByDefault:
						  SSEAlgorithm: aws:kms
						  KMSMasterKeyID: alias/aws/s3
				  PublicAccessBlockConfiguration:
					IgnorePublicAcls: true
					RestrictPublicBuckets: true
			  BucketBucketPolicy:
				Type: AWS::S3::BucketPolicy
				Properties:
				  Bucket: !Ref Bucket
				  PolicyDocument:
					Id: RequireEncryptionInTransit
					Version: '2012-10-17'
					Statement:
					  - Principal: '*'
						Action: '*'
						Effect: Deny
						Resource:
						  - !GetAtt Bucket.Arn
						  - !Sub ${Bucket.Arn}/*
						Condition:
						  Bool:
							aws:SecureTransport: 'false'		
							
							
		Infra-Structure Configuration ::::
		
		
			CloudFormation Template : UserData 
		
		
	Elastic BeanStalk :::
		- PAAS
		
		- Fully Managed PaaS 
		
		- Security Patch Updates in all the target servers 
		
		- Support various language platforms :
		
			- Python,Ruby,PHP,Go,COBOL,Java,.Net,Docker,Tomcat,Nginx,httpd 

			- WAS - Web Application Servers : 
			
			- Load-Balancer --> 
			
	
		Elastic BeanStalk Components :
		
			- Application 
			
			- Application Version 
					- Artifactory Libraries 
					
					- Packaged as *.zip_v1.0
					
			- Environment 
			
				Non-Prod  ==> Prod 
	
			- Environment Health :
			
				--> Health Checker --> Monitoring 
				
				--> Elastic BeanStalk Health Monitor 
				
					-- Diff Color Codes 
					
					
		Architecture of Elastic BeanStalk :
			
			--> Elastic BeanStalk Environments
			--> Elastic Load-Balancer
			--> Auto Scaling Group 
			
			--> EC2 Instances 
			
			--> Security Groups 
		
			--> Storage 
				--> EBS / EFS / MSG Queue
				
			--> Docker Image 
			
			
		Elastic BeanStalk Proces Workflow ::::
		
			--> S3 Bucket 
			--> Elastic BeansStack Project 
			--> Security Group 
			--> Auto Scaling Group 
			--> Auto Scaling Group Policies 
			--> CloudWatch Alarms 
			--> Load Balancer 
			--> EC2 Instances 
			--> Applications Environments
			
	
		AWS OpsWork --> Configuration Management Tool :::  --> stopped this service!		
		
	Kubernetes-Master(Prod)
		Kubernetes-WorkerNode1,2,3,4,5
		
	
		Kubernetes :::

			- Kubernetes is an Open-Source Container Orchestration Tool 
			- To ensure high availability of Containerized Applications
			- By create replicas
			- Auto-Scale : Scale-up/Scale-Down
			- Load-Balancing 
			- Self-Healing 
			- Used to Achieve Upgrade or Downgrade without any downtime 

	
			sign_in service 
	
				-> Front-End Layer 		# User Interaction		C1.1,C1.2,C1.3
				
				-> Application Layer 	# Business Logics 		C2.1,C2.2,C2.3
				
				-> Back-End Layer 		# Database 				C3.1,C3.2,C3.3 	


		Core Concepts of Kubernetes :
		
			- Pods 					# Atomic unit of scheduling, Pod executes the Containers within 
			
			- Contianer Image 
			
			- Container 
			
			- Container Registry 
			
			- kubeadm 				# Command Line Utility to Configure Kubernetes Cluster 

			- kubectl				# Command Line Utility to Interact with Kubernetes Master 
			
			- Kubernetes-Cluster 	# Collection of WorkerNodes 
	
	
				Kubernetes-Master(Prod)					# To Create and Schedule the Pods for Deployments 
				
					Kubernetes-WorkerNode1,2,3,4,5		# To run the Application Pods 	
					
			7 Replicas of Pods :
			
			
			Kubernetes-Master 
			
				Kubernetes-Cluster					Logical grouping of WorkerNodes 
					Kubernetes-WorkerNode1			Physical Entity	to run the pod  *.war --> App_Img --> Pod(Container) 
					Kubernetes-WorkerNode2
					Kubernetes-WorkerNode3
					Kubernetes-WorkerNode4
					Kubernetes-WorkerNode5
		

			Kubernetes-Master 								On-Prem 
		
				Kubernetes-Master 								AWS 
				
					Kubernetes-Cluster1							AWS Region1
						Kubernetes-WorkerNode1
						Kubernetes-WorkerNode2
						Kubernetes-WorkerNode3
				
					Kubernetes-Cluster2							AWS Region2
						Kubernetes-WorkerNode1
						Kubernetes-WorkerNode2
						Kubernetes-WorkerNode3			
				
					Kubernetes-Cluster3							AWS Region3
						Kubernetes-WorkerNode1
						Kubernetes-WorkerNode2
						Kubernetes-WorkerNode3				
			
				Kubernetes-Master 								Azure  
				
					Kubernetes-Cluster1							Azure Region1
						Kubernetes-WorkerNode1
						Kubernetes-WorkerNode2
						Kubernetes-WorkerNode3
				
					Kubernetes-Cluster2							Azure Region2
						Kubernetes-WorkerNode1
						Kubernetes-WorkerNode2
						Kubernetes-WorkerNode3			
				
					Kubernetes-Cluster3							Azure Region3
						Kubernetes-WorkerNode1
						Kubernetes-WorkerNode2
						Kubernetes-WorkerNode3							
						
			
		

		
########################
Day 7 - 22nd Mar. 2025
########################			
					
	-> Continuous Monitoring :::
	
			- Is used to achieve business continuity..
			
			- Continuously Monitor the Infra-Structure and the Applications running in the Production Environment
			
			- Completely owned and managed by Infra-Structure Monitoring Team. 
			
			- Infra-Structure Monitoring 
			
				-  Production Server :
				
						--> CPU Utilization 
						
						--> Memory Utilization 
						
						--> Network Traffic 
						
						
					Monitoring Tools :
					
						- Prometheus/Grafana/Splunk/Dynatrace/Kafka/Nagios
						- AWS - AWS CloudWatch 
						
							-> 75% of threshold Limit
							
							-> Alert the Users (Production Support/Stakeholder)
						
			
			- Application Monitoring ::
			
				--> AppDynamics / DataDog	
				
	
	Environments :::
	
		Non-Prod 															Prod Environments 
		
			Kubernetes-Cluster(Non-Prod)(Business Hours)
		
				Dev 
		
				QA 
		
				UAT 								==========>					Kubernetes-Cluster(Prod)(24/7*)
																					- Active(LIVE) / Passive Environments
		
	Architecture of Monitoring Tools :
	
		- Monitoring Server :(VM) 
		
			Server(Monitoring Server) and Clients(Target servers) 
			
		- Monitoring Agent should be installed in the Target Server 
		
		- Database (TimeSeries DataBase)
		
	
	AWS Cloud Watch ==> Kubernetes-Cluster (EKS)
	
	Kubernetes-Cluster(Open-Source) ==> Prometheus/Grafana
	
	
	Prometheus/Grafana/Splunk/Dynatrace/Kafka/Nagios :
	
	
		DB_Server -> Onprem 
	
		Web Application Servers (AWS)	--> AWS CloudWatch(SaaS)
		
		Web Application Servers (Azure)	--> Azure Monitor	
	
	
	Logging/Reporting Tools :::
	
	Online - E_Commerce :
	
		Infra-Structure --> To ensure the Servers/Resources Availability 
		
		
		User Transaction --> signin,prod,payment(no response)

	Monitoring Tools :::
	
		Install and Configure the Monitoring Tools :::
		
		
	CICD Pipeline :
	
		Builds 		(frequently) (Temp Build Environment) ==> Result based on the artifacts 
		
					(Attach Cloud Watch Monitoring Log) == To create log Stream
					
		Prometheus/Grafana/Splunk/Dynatrace/Kafka/Nagios  -- 3rd Party Monitoring Tools :::
		
		
		Dedicated Monitoring Server (Dbase)	==> query the logs and send to the users thru email/Dashboard(grafana)
		
		VM - Install the Monitoring Tool.
		
	
	Prometheus & Grafana :
	
	
	https://prometheus.io/download/
	
	
	
	Monitoring : Kubernetes-Cluster
	
	Prod Server: 3 - 5 
	
		Target Servers :
		
	
		Kubernetes-Cluster :
		
			Kubernetes-WorkerNode1,2,3,4,5,6,7,8,9,...............,175
	
	
	
	Execute Prometheus & Grafana as pods.
	
	Using Helm Charts we can configure the Monitoring Services 
	
	Helm Charts	# Package Manager for Kubernetes
	
	
	- Create/Setup Kubernetes-Cluster
	
	- Deploy the Pods
	
	- Helm Charts to Install and Configure Monitoring Tools & Dashboards 
	
	- Daemonsets 		# Used to Execute Prometheus/Grafana in Kubernetes-Cluster for monitoring 

Next ::

	- Deploy the Pods
	
	- Helm Charts to Install and Configure Monitoring Tools & Dashboards 			
					
	

	
########################
Day 8 - 23rd Mar. 2025
########################	
	
	
	CICD : 
	
	
	Kubernetes Manifest File :
	
	
	Kubernetes Deployment Controller Object :
	
		- Deploy the Pods using pod replicas 
		- It ensure high availability of pods by creating replicas of pods. 
		- Deployment object will automaticall create replicaset.
		- Auto-Scaling : Scale-up/Scale-Down
		- Self-Healing 
		- Ensure ZERO Downtime during Application Upgrade/Downgrade.
		- Deployment Controller Object uses Deployment Strategy : Rolling Update Strategy
		
		Horizontal Scaling 			/ 	Vertical Scaling 
		
		
		
		1. Deployment Object 
		
		2. Replicaset 
		
		3. Pod Instance 
		
		
		
		App_Service :
		
			1000 Users  	-	 100 Instance of Pods(replicas)
		
			10000 Users
		
		
		
		Continuous Deployment :::
	
				- Continuous Deployment can be achieved only with the Micro-Service Based Application Architecture	
				
				
		
	App_Service1	:		2 Replicas of Pods 
	
	
	
		App_Service1.war_v1.0		====>			App_Service1_Img:v1.0
		
		App_Service1.war_v2.0		====>			App_Service1_Img:v2.0	
	
	
	
		Deployment Controller Object uses Deployment Strategy : Rolling Update Strategy
	
	
		p1 	App_Service1_Img:v1.0			p1.1	App_Service1_Img:v2.0
		
		p2	App_Service1_Img:v1.0			p2.1	App_Service1_Img:v2.0
		
		p3 	App_Service1_Img:v1.0			p3.1	App_Service1_Img:v2.0
	
	
	
	Monitoring Kubernetes :::::
	
		
		Use Prometheus/Grafana :::
	
		Helm Package Manager :::
		
		
		Install Helm 
		
		Using Helm 
		
		Install Prometheus/Grafana Package in Kubernetes
		
		Linux Package Managers :
		
			yum / apt / dnf 	-- Used to manage the packages in specific Linux Node
		
		
		HELM Chart:::
		Helm is a pkg manager for kubernetes which help to automatically create manifest files for any kubernetes objects in the form of HELM Charts
	
	
		13.234.76.2:31259
		
		31259
		
	
		
		RBAC :::
		
			Role Base Access Control :
			
				--> Namespace Level : based on specific team/environment 
				
				--> Cluster Level : includes all Namespaces 
				
		Kubernetes Namespace :
			- Logical partition of Kubernetes-Cluster 
			
			
			
			Kubernetes-Master 								Namespace : dev / qa / uat 
				Kubernetes-WorkerNode1,2,3,4
				
				
				
		c:/ 
		
			dev : 
					
				*.html 			>>> Secret Volume : Security Vaults 
				*.java 
				*.doc 
				*.mp3
		

			qa : 
					
				*.html 
				*.java 
				*.doc 
				*.mp3
				
		
		Source Code :
		
			Scan for Code Quality -- SonarQube - Static Code Analyzer


	Dev 
	
		Create Source-Code 
			Scan for Code Quality -- SonarQube - Static Code Analyzer
				- Code Coverage 
				- Test Coverage 

	
	SCM_Checkout
	
	Application_Build 
			- Create Artifacts 
			- Unit Testing 
			- Sonarqube Analysis --> Code Quality and Code Coverage - Test Coverage
			
	Application_Container_Image
			- Container Image 
		
		FROM ubuntu
		
	
DevOps Path:

-> AWS DevOps Service : ****

-> Azure DevOps Services :

-> Github Repo & GitHub Actions - To Create CICD Pipelines :
					
-> Gitlab Repo & GitLab-CI - To Create CICD Pipelines :
	
			
		
